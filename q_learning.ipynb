{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 100\n",
    "train_batches_per_episode = 100\n",
    "batch_size = 128\n",
    "target_copy_freq = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "state_dim = 4\n",
    "n_actions = 2\n",
    "\n",
    "buffer = [] # Replay buffer to hold environment transitions\n",
    "training_rewards = []\n",
    "rng = np.random.RandomState(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(state_dim, 128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(128, n_actions))\n",
    "\n",
    "target_net = nn.Sequential(nn.Linear(state_dim, 128),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Linear(128, n_actions))\n",
    "target_net.load_state_dict(net.state_dict())\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "#optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(s):\n",
    "    action_values = net.forward(torch.Tensor(s.reshape((1, -1))))\n",
    "    best_action = action_values.max(dim=1)[1].numpy()\n",
    "    if rng.uniform() < 0.95:\n",
    "        return best_action[0]\n",
    "    else:\n",
    "        # 5% chance of random action to encourage exploration\n",
    "        return rng.choice(n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in tqdm(range(num_episodes)):\n",
    "    # Do a rollout\n",
    "    s = env.reset()\n",
    "    ep_reward = 0\n",
    "    while True:\n",
    "        a = policy(s)\n",
    "        s_new, r, d, _ = env.step(a)\n",
    "        #r = r - 0.05*((s[0]-0.3)**2)  # Optional reward shaping\n",
    "        buffer.append((s, a, s_new, r, d))\n",
    "        s = s_new\n",
    "        ep_reward += r\n",
    "        if d:\n",
    "            training_rewards.append(ep_reward)\n",
    "            break\n",
    "    # Train\n",
    "    if len(buffer) >= batch_size: # Ensure we have enough data for a full batch\n",
    "        for train_step in range(train_batches_per_episode):\n",
    "            idx = rng.choice(len(buffer), replace=False, size=batch_size) # Sample randomly from the buffer\n",
    "            # Create pytorch tensors for the data in this batch\n",
    "            s = torch.Tensor([buffer[i][0] for i in idx])\n",
    "            a = torch.Tensor([buffer[i][1] for i in idx]).long()\n",
    "            s_new = torch.Tensor([buffer[i][2] for i in idx])\n",
    "            r = torch.Tensor([buffer[i][3] for i in idx])\n",
    "            d = torch.Tensor([buffer[i][4] for i in idx]).float()\n",
    "            \n",
    "            Q_values = target_net.forward(s_new)\n",
    "            max_Qs = Q_values.max(dim=1)[0] # Get max over actions.\n",
    "            targets = r + (1 - d) * max_Qs # Necessary so that we treat done states as having zero future value\n",
    "            targets = targets.detach() # Detach prevents back-prop of gradients\n",
    "            \n",
    "            lhs_Q_vals = net.forward(s)[torch.arange(batch_size), a] # LHS Q-values for actions that were taken\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(lhs_Q_vals, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Periodically update the target network with the new parameters\n",
    "            if train_step + ep*train_batches_per_episode % target_copy_freq == 0:\n",
    "                target_net.load_state_dict(net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_rewards)\n",
    "plt.xlabel('Episodes run')\n",
    "plt.ylabel('Episode length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a video to a directory\n",
    "env_to_wrap = gym.make('CartPole-v0')\n",
    "env = gym.wrappers.Monitor(env_to_wrap, 'video_output', force = True)\n",
    "s = env.reset()\n",
    "while True:\n",
    "    s, _, d, _ = env.step(policy(s))\n",
    "    print(s)\n",
    "    if d:\n",
    "        break\n",
    "env.close()\n",
    "env_to_wrap.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
